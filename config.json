{
    "scripts": [
        {
            "name": "mySql_logParser",
            "display_name": "Mysql DB log parser",
            "description": "Parses Mysql DB logs and shares excel report",
            "help_text": "### Purpose\nThis script processes a MySQL log file to extract and analyze query performance metrics, such as query execution time, rows examined, and rows sent. It also normalizes queries, aggregates results, and suggests potential indexes for optimization. The results are saved into an Excel file for further analysis.\n\n### Key Features\n1. **Dependencies**:\n   - Uses libraries like `pandas` for data manipulation, `re` for regex operations, `sqlparse` for parsing SQL queries, and `argparse` for handling command-line arguments.\n\n2. **Normalization Function**:\n   - The `normalize_query` function removes specific values (e.g., literals, numbers) from SQL queries and converts them to uppercase for consistency. This helps in grouping similar queries for analysis.\n\n3. **Log Parsing**:\n   - The `parse_mysql_log` function extracts key metrics from the MySQL log file, such as:\n     - Query execution time (`Query_time`).\n     - Lock time (`Lock_time`).\n     - Rows sent (`Rows_sent`).\n     - Rows examined (`Rows_examined`).\n     - Original and normalized queries.\n\n4. **Data Aggregation**:\n   - Aggregates query metrics by normalized queries, calculating:\n     - Execution count.\n     - Minimum, maximum, and average query execution times.\n     - A sampl...",
            "inputs": [
                {
                    "type": "file",
                    "label": "Upload MySQL Log File(s)",
                    "name": "input",
                    "required": true,
                    "multiple_files": true,
                    "allowed_extensions": [
                        ".log",
                        ".txt"
                    ]
                }
            ],
            "output_type": "xlsx",
            "script_path": "scripts/mySql_logParser.py"
        },
        {
            "name": "straceAnalyzer",
            "display_name": "Strace Analyzer",
            "description": "Analyzes strace log files to identify system call patterns, performance bottlenecks, and language-specific behaviors.",
            "help_text": "### Purpose\nThis tool parses and analyzes `strace` log files, providing insights into the system calls made by a process. It helps identify performance bottlenecks, common errors, and now includes language-specific analysis for processes written in Java, Python, or Node.js.\n\n### How to Collect Log Files (Helpful Commands)\nTo use this tool effectively, capture the output of `strace` and save it to a `.log` or `.txt` file.\n\n* **Recommended `strace` command for detailed analysis:**\n    `strace -T -o strace_log.txt <your_command_to_trace>`\n    -   `-T`: Show the time spent in each syscall.\n    -   `-o strace_log.txt`: Redirect output to a file.\n    -   Replace `<your_command_to_trace>` with the actual command you want to trace (e.g., `java -jar myapp.jar`, `python myscript.py`, `node app.js`, or any other executable).\n\n* **Example for a Java application:**\n    `strace -T -o java_strace.log java -jar /path/to/your/application.jar`\n\n* **Example for a Python script:**\n    `strace -T -o python_strace.log python /path/to/your/script.py`\n\n* **Example for a Node.js application:**\n    `strace -T -o node_strace.log node /path/to/your/app.js`\n\n### Usage in this Tool\n1.  **Upload File:** Click 'Browse files' to upload your `.log` or `.txt` file containing the `strace` output.\n2.  The tool will parse the data, display a preview of the raw parsed data, and provide a comprehensive summary analysis, including:\n    -   Most frequent and longest-running system calls.\n    -   Identification of system calls returning errors.\n    -   Summary of I/O and process management related syscalls.\n    -   **Detection of the likely programming language (Java, Python, Node.js, or unknown) and language-specific insights.**\n3.  If AI Analysis is enabled, it will provide an AI-driven interpretation of the `strace` data and suggest actionable recommendations.\n4.  You can download the parsed data as a CSV file for further offline analysis.",
            "inputs": [
                {
                    "type": "file",
                    "label": "Upload strace Log File(s)",
                    "name": "log_file",
                    "required": true,
                    "multiple_files": true,
                    "allowed_extensions": [
                        ".log",
                        ".txt"
                    ]
                }
            ],
            "output_type": "streamlit_app",
            "script_path": "scripts/strace_analyzer.py",
            "ai_prompt": {
                "system_role": "You are an expert in Linux system performance, debugging, and software architecture, specializing in analyzing strace output. Your task is to analyze the provided strace summary and raw data, identify potential issues, and suggest targeted solutions.",
                "task_description": "Analyze the strace data summary. The process might be written in Java, Python, Node.js, or other languages. Focus on identifying bottlenecks, unusual behavior, resource contention, and potential errors based on syscall patterns, durations, and arguments. Pay special attention to language-specific syscall patterns.",
                "analysis_points": [
                    "Identify the most frequent and longest-running system calls. What do these indicate about the process's primary activities and potential areas of contention or waiting?",
                    "Detect patterns of I/O (file, network) and process/thread creation. Are there signs of I/O saturation, excessive process/thread spawning, or inefficient resource handling?",
                    "Look for system calls returning errors (e.g., EACCES for permission issues, ENOENT for missing files, EAGAIN for resource contention, EPIPE for broken pipes, EINVAL for invalid arguments). What do these errors suggest about misconfigurations, missing resources, race conditions, or programming bugs?",
                    "Analyze the detected programming language. For **Java/JVM** processes, identify frequent `mmap`/`munmap` (JIT compilation, memory allocation), `openat` calls to `.class` files or JARs (class loading), and `libjvm.so` references (JNI calls). Look for GC-related pauses if duration data is available.\nFor **Python** processes, detect `openat` calls to `.py` files (module imports) and loading of `.so` extension modules. Track potential GIL-related blocking if applicable.\nFor **Node.js** processes, note `libnode.so` and `libv8.so` references (V8 engine activity) and `node_modules` access. Assess event loop delays if possible from durations.",
                    "Assess memory-related syscalls (e.g., `mmap`, `munmap`, `brk`, `madvise`) for signs of memory pressure, fragmentation, or excessive memory requests.",
                    "Summarize the overall health and behavior of the traced process, highlighting critical findings.",
                    "Propose clear, actionable, and prioritized recommendations to optimize performance or resolve identified issues. For language-specific processes, suggest relevant tuning parameters (e.g., JVM options, Python environment optimizations, Node.js event loop best practices) or deeper debugging steps. Also, consider recommendations for handling language-specific error patterns (e.g., Java ClassNotFound, Python ImportError)."
                ],
                "output_format": "Provide a comprehensive, structured analysis in markdown format. Start with an **Executive Summary** highlighting the most critical findings. Follow with detailed findings for each category (Syscall Frequency, Duration, Errors, I/O, Process Management, **Language-Specific Analysis**). Conclude with clear, prioritized, and actionable **Recommendations**. Include numerical evidence from the summary to support your points and reference specific syscalls or error types where appropriate."
            }
        },
        {
    "name": "threadDumpAnalyzer",
    "display_name": "Thread Dump Analyzer",
    "description": "Analyzes Java thread dump files to identify deadlocks, thread contention, performance bottlenecks, and application-specific threading patterns.",
    "help_text": "### Purpose\nThis tool parses and analyzes Java thread dump files, providing insights into thread states, lock contention, deadlocks, and performance bottlenecks. It helps identify threading issues, blocked threads, and resource contention patterns in Java applications.\n\n### How to Collect Thread Dumps (Helpful Commands)\nTo use this tool effectively, capture thread dumps from your Java application and save them to `.txt` or `.dump` files.\n\n* **Using jstack (recommended for most cases):**\n    `jstack <pid> > threaddump_$(date +%Y%m%d_%H%M%S).txt`\n    -   Replace `<pid>` with your Java process ID (find using `jps` or `ps aux | grep java`)\n    -   Automatically timestamps the dump file\n\n* **Using kill -3 (sends SIGQUIT to JVM):**\n    `kill -3 <pid>`\n    -   Thread dump will be written to the application's stdout/stderr or log files\n    -   Less intrusive than jstack but output location varies\n\n* **Using jcmd (Java 7+):**\n    `jcmd <pid> Thread.print > threaddump.txt`\n    -   More comprehensive output than jstack\n    -   Includes additional JVM information\n\n* **For multiple dumps (recommended for analysis):**\n    ```bash\n    for i in {1..5}; do\n        jstack <pid> > threaddump_${i}_$(date +%H%M%S).txt\n        sleep 10\n    done\n    ```\n    -   Captures 5 dumps with 10-second intervals\n    -   Helps identify persistent vs. transient issues\n\n* **Using JVisualVM or JConsole:**\n    -   GUI tools that can generate and save thread dumps\n    -   Useful for development and testing environments\n\n* **Application server specific:**\n    -   **Tomcat:** `<CATALINA_HOME>/bin/catalina.sh threaddump`\n    -   **WebLogic:** Use Admin Console or WLST commands\n    -   **WebSphere:** Use wsadmin or IBM Support Assistant\n\n### Usage in this Tool\n1.  **Upload File:** Click 'Browse files' to upload your `.txt` or `.dump` files containing thread dump data.\n2.  The tool will parse the data, display a preview of threads and their states, and provide comprehensive analysis including:\n    -   Thread state distribution (RUNNABLE, BLOCKED, WAITING, etc.)\n    -   Deadlock detection and analysis\n    -   Lock contention hotspots and waiting patterns\n    -   Thread pool analysis and utilization\n    -   **Application framework detection (Spring, Hibernate, Tomcat, etc.) and framework-specific insights**\n    -   CPU-intensive vs. I/O-bound thread identification\n3.  If AI Analysis is enabled, it will provide intelligent interpretation of threading patterns and suggest specific optimization strategies.\n4.  You can download the parsed analysis as a CSV or JSON file for further investigation or sharing with development teams.",
    "inputs": [
        {
            "type": "file",
            "label": "Upload Thread Dump File(s)",
            "name": "dump_file",
            "required": true,
            "multiple_files": true,
            "allowed_extensions": [
                ".txt",
                ".dump",
                ".log"
            ]
        }
    ],
    "output_type": "streamlit_app",
    "script_path": "scripts/threaddump_analyser.py",
    "ai_prompt": {
        "system_role": "You are an expert in Java performance tuning, concurrent programming, and JVM internals, specializing in analyzing thread dumps. Your task is to analyze thread dump data, identify threading issues, deadlocks, and performance bottlenecks, and provide actionable optimization recommendations.",
        "task_description": "Analyze the thread dump data summary. The application may use various frameworks like Spring, Hibernate, Tomcat, or custom threading models. Focus on identifying deadlocks, thread contention, resource bottlenecks, and inefficient threading patterns based on thread states, stack traces, and lock information.",
        "analysis_points": [
            "Identify and analyze thread state distribution (RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, NEW, TERMINATED). What does the distribution indicate about application behavior and potential bottlenecks?",
            "Detect deadlocks by analyzing circular wait conditions and lock dependencies. Identify the threads and resources involved, and trace the deadlock chain. Explain the root cause and impact.",
            "Analyze lock contention patterns by identifying threads waiting on the same monitors or locks. Look for synchronized blocks, ReentrantLocks, and other concurrency primitives causing bottlenecks.",
            "Examine thread pool utilization and patterns. Identify thread pools (Executor services, application server pools, framework-specific pools) and assess their efficiency, queue saturation, and optimal sizing.",
            "Detect framework-specific patterns: **Spring** (transaction managers, AOP proxies, async processing), **Hibernate** (session management, connection pools, lazy loading), **Tomcat** (HTTP connector threads, session management), **Netty** (event loops, channel handlers), and **custom frameworks**. Identify framework-specific anti-patterns.",
            "Analyze CPU-intensive vs. I/O-bound thread behavior. Identify threads performing heavy computation, database operations, network I/O, or file system access. Look for threads stuck in long-running operations.",
            "Examine garbage collection impact on threading. Identify GC-related pauses and their effect on application threads, especially in high-throughput scenarios.",
            "Look for resource leaks and inefficient patterns: unclosed resources, thread leaks, excessive thread creation, inappropriate use of thread pools, or blocking operations on critical threads.",
            "Assess application-specific threading patterns and identify opportunities for optimization, such as better thread pool configuration, asynchronous processing, or lock-free algorithms.",
            "Summarize the overall threading health of the application, highlighting critical issues that need immediate attention vs. optimization opportunities."
        ],
        "output_format": "Provide a comprehensive, structured analysis in markdown format. Start with an **Executive Summary** highlighting the most critical threading issues and their business impact. Follow with detailed sections: **Thread State Analysis**, **Deadlock Detection**, **Lock Contention Analysis**, **Thread Pool Assessment**, **Framework-Specific Findings**, **Performance Bottlenecks**, and **Resource Utilization**. Conclude with **Prioritized Recommendations** that include specific configuration changes, code improvements, and monitoring suggestions. Include thread counts, percentages, and specific thread names/stack traces to support findings. Provide actionable solutions with expected impact levels (High/Medium/Low)."
    }
},
        {
            "name": "mongoGCPLogParser",
            "display_name": "MongoDB GCP Log Parser",
            "description": "Parses MongoDB GCP audit logs and provides a structured Excel report.",
            "help_text": "### Purpose\nThis script is designed to parse MongoDB GCP audit log files, extract key information such as operations, users, databases, and timestamps, and generate a structured Excel report. This tool is useful for database administrators and developers who want to analyze MongoDB performance logs and identify slow queries or errors in a structured format.\n",
            "inputs": [
                {
                    "type": "file",
                    "label": "Upload Logs",
                    "name": "input",
                    "required": true,
                    "multiple_files": true,
                    "allowed_extensions": [
                        ".csv",
                        ".log",
                        ".txt"
                    ]
                }
            ],
            "output_type": "xlsx",
            "script_path": "scripts/mongoGCPLogParser.py"
        },
        {
            "name": "postgresGCPLogParser",
            "display_name": "Postgres GCP log parser",
            "description": "Parses postgres logs and shares excel report",
            "help_text": "### Purpose\nThis script processes a postgres log file to extract and analyze metrics related to slow queries, errors, and other performance-related data. The results are then output into an Excel file for further analysis.",
            "inputs": [
                {
                    "type": "file",
                    "label": "Upload Logs",
                    "name": "input",
                    "ai-analysis": true,
                    "required": true,
                    "multiple_files": true,
                    "allowed_extensions": [
                        ".csv",
                        ".log",
                        ".txt"
                    ]
                }
            ],
            "output_type": "xlsx",
            "script_path": "scripts/parse_postgres_log.py"
        }
    ]
}